{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2456446\n"
     ]
    }
   ],
   "source": [
    "data_train = []\n",
    "with open('PIZZA_train.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data_train.append(json.loads(line))\n",
    "\n",
    "print(len(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev.SRC': 'i want to order two medium pizzas with sausage and black olives and two medium pizzas with pepperoni and extra cheese and three large pizzas with pepperoni and sausage', 'dev.EXR': '(ORDER (PIZZAORDER (NUMBER 2 ) (SIZE MEDIUM ) (COMPLEX_TOPPING (QUANTITY EXTRA ) (TOPPING CHEESE ) ) (TOPPING PEPPERONI ) ) (PIZZAORDER (NUMBER 2 ) (SIZE MEDIUM ) (TOPPING OLIVES ) (TOPPING SAUSAGE ) ) (PIZZAORDER (NUMBER 3 ) (SIZE LARGE ) (TOPPING PEPPERONI ) (TOPPING SAUSAGE ) ) )', 'dev.TOP': '(ORDER i want to order (PIZZAORDER (NUMBER two ) (SIZE medium ) pizzas with (TOPPING sausage ) and (TOPPING black olives ) ) and (PIZZAORDER (NUMBER two ) (SIZE medium ) pizzas with (TOPPING pepperoni ) and (COMPLEX_TOPPING (QUANTITY extra ) (TOPPING cheese ) ) ) and (PIZZAORDER (NUMBER three ) (SIZE large ) pizzas with (TOPPING pepperoni ) and (TOPPING sausage ) ) )', 'dev.PCFG_ERR': 'False'}\n"
     ]
    }
   ],
   "source": [
    "with open('PIZZA_dev.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the loaded data\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_train(data):\n",
    "    return data['train.SRC'], data['train.EXR'], data['train.TOP'], data['train.TOP-DECOUPLED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(data):\n",
    "    return data['dev.SRC'], data['dev.EXR'], data['dev.TOP'], data['dev.PCFG_ERR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, explanations, topics, decoupled_topics = map(\n",
    "    np.array, zip(*map(extract_text, data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_t, explanations_t, topics_t, decoupled_topics_t = map(\n",
    "    np.array, zip(*map(extract_text_train, data_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = set([word[1:]\n",
    "               for t in topics for word in t.split() if word.isupper()])\n",
    "\n",
    "entities_train = set([word[1:]\n",
    "               for t in topics_t for word in t.split() if word.isupper()])\n",
    "\n",
    "full_entities = entities | entities_train\n",
    "\n",
    "enitities_exclude_not = entities - {'NOT'}\n",
    "\n",
    "negate_mapping = {}\n",
    "for entity in entities:\n",
    "    if entity != 'NOT':\n",
    "        negate_mapping[entity] = \"NOT_\" + entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'QUANTITY', 'TOPPING', 'PIZZAORDER', 'STYLE', 'NUMBER', 'COMPLEX_TOPPING', 'SIZE', 'DRINKORDER', 'DRINKTYPE', 'CONTAINERTYPE', 'NOT', 'ORDER'}\n",
      "{'QUANTITY', 'TOPPING', 'PIZZAORDER', 'STYLE', 'NUMBER', 'COMPLEX_TOPPING', 'SIZE', 'DRINKORDER', 'DRINKTYPE', 'CONTAINERTYPE', 'VOLUME', 'NOT', 'ORDER'}\n",
      "{'TOPPING', 'PIZZAORDER', 'NUMBER', 'SIZE', 'DRINKORDER', 'CONTAINERTYPE', 'VOLUME', 'QUANTITY', 'STYLE', 'COMPLEX_TOPPING', 'DRINKTYPE', 'NOT', 'ORDER'}\n",
      "{'QUANTITY', 'TOPPING', 'PIZZAORDER', 'STYLE', 'NUMBER', 'COMPLEX_TOPPING', 'SIZE', 'DRINKORDER', 'DRINKTYPE', 'CONTAINERTYPE', 'ORDER'}\n",
      "{'QUANTITY': 'NOT_QUANTITY', 'TOPPING': 'NOT_TOPPING', 'PIZZAORDER': 'NOT_PIZZAORDER', 'STYLE': 'NOT_STYLE', 'NUMBER': 'NOT_NUMBER', 'COMPLEX_TOPPING': 'NOT_COMPLEX_TOPPING', 'SIZE': 'NOT_SIZE', 'DRINKORDER': 'NOT_DRINKORDER', 'DRINKTYPE': 'NOT_DRINKTYPE', 'CONTAINERTYPE': 'NOT_CONTAINERTYPE', 'ORDER': 'NOT_ORDER'}\n"
     ]
    }
   ],
   "source": [
    "print(entities)\n",
    "print(entities_train)\n",
    "print(full_entities)\n",
    "print(enitities_exclude_not)\n",
    "print(negate_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negate_topic(topic):\n",
    "    not_array = topic.split('NOT')\n",
    "    not_array = not_array[1:]\n",
    "    final_array = []\n",
    "    for element in not_array:\n",
    "        element = element.strip()\n",
    "        open_brackets = 1\n",
    "        result = \"\"\n",
    "        index = 0\n",
    "        while open_brackets > 0:\n",
    "            if element[index] == '(':\n",
    "                open_brackets += 1\n",
    "            elif element[index] == ')':\n",
    "                open_brackets -= 1\n",
    "            if open_brackets > 0:\n",
    "                result += element[index]\n",
    "            index += 1\n",
    "\n",
    "        final_array.append(result.strip())\n",
    "    return final_array\n",
    "\n",
    "\n",
    "def apply_negation(topic):\n",
    "    array = negate_topic(topic)\n",
    "    for el in array:\n",
    "        original_el = el\n",
    "        for entity in enitities_exclude_not:\n",
    "            el = el.replace(entity, negate_mapping[entity])\n",
    "            \n",
    "        topic = topic.replace(original_el, el)\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(TOPPING bacon )']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negate_topic(topics[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(ORDER i want (PIZZAORDER (NUMBER a ) (TOPPING sausage ) pie with (COMPLEX_TOPPING (QUANTITY extra ) (TOPPING cheese ) ) no (NOT (NOT_TOPPING bacon ) ) ) )'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_negation(topics[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "negated_topics = np.vectorize(apply_negation)(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.str_('(ORDER i want (PIZZAORDER (NUMBER a ) (TOPPING sausage ) pie with (COMPLEX_TOPPING (QUANTITY extra ) (TOPPING cheese ) ) no (NOT (NOT_TOPPING bacon ) ) ) )')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negated_topics[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.str_('(ORDER i want (PIZZAORDER (NUMBER a ) (SIZE lunch size ) pizza with no (NOT (TOPPING apple wood bacon ) ) ) )')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COMPLEX_TOPPING',\n",
       " 'CONTAINERTYPE',\n",
       " 'DRINKORDER',\n",
       " 'DRINKTYPE',\n",
       " 'NOT',\n",
       " 'NUMBER',\n",
       " 'ORDER',\n",
       " 'PIZZAORDER',\n",
       " 'QUANTITY',\n",
       " 'SIZE',\n",
       " 'STYLE',\n",
       " 'TOPPING'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COMPLEX_TOPPING',\n",
       " 'CONTAINERTYPE',\n",
       " 'DRINKORDER',\n",
       " 'DRINKTYPE',\n",
       " 'NOT',\n",
       " 'NUMBER',\n",
       " 'ORDER',\n",
       " 'PIZZAORDER',\n",
       " 'QUANTITY',\n",
       " 'SIZE',\n",
       " 'STYLE',\n",
       " 'TOPPING',\n",
       " 'VOLUME'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'QUANTITY', 'TOPPING', 'STYLE', 'NUMBER', 'SIZE', 'DRINKTYPE', 'CONTAINERTYPE', 'VOLUME'}\n"
     ]
    }
   ],
   "source": [
    "entities_copy = full_entities.copy()\n",
    "entities_copy.remove('NOT')\n",
    "entities_copy.remove('COMPLEX_TOPPING')\n",
    "entities_copy.remove('PIZZAORDER')\n",
    "entities_copy.remove('DRINKORDER')\n",
    "entities_copy.remove('ORDER')\n",
    "print(entities_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'QUANTITY': 'NOT_QUANTITY', 'TOPPING': 'NOT_TOPPING', 'STYLE': 'NOT_STYLE', 'NUMBER': 'NOT_NUMBER', 'SIZE': 'NOT_SIZE', 'DRINKTYPE': 'NOT_DRINKTYPE', 'CONTAINERTYPE': 'NOT_CONTAINERTYPE', 'VOLUME': 'NOT_VOLUME'}\n"
     ]
    }
   ],
   "source": [
    "negate_mapping = {}\n",
    "for entity in entities_copy:\n",
    "    if entity != 'NOT':        negate_mapping[entity] = \"NOT_\" + entity\n",
    "\n",
    "print(negate_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NOT_SIZE', 'SIZE', 'NOT_CONTAINERTYPE', 'VOLUME', 'NOT_STYLE', 'NOT_QUANTITY', 'TOPPING', 'CONTAINERTYPE', 'NOT_TOPPING', 'NOT_VOLUME', 'STYLE', 'NOT_DRINKTYPE', 'NOT_NUMBER', 'DRINKTYPE', 'NUMBER', 'QUANTITY'}\n"
     ]
    }
   ],
   "source": [
    "full_negate_entities= set(negate_mapping.values())|set(entities_copy)\n",
    "np.save('full_negate_entities.npy', list(full_negate_entities))\n",
    "print(full_negate_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SIZE', 'DRINKORDER', 'TOPPING', 'ORDER', 'CONTAINERTYPE', 'PIZZAORDER', 'STYLE', 'COMPLEX_TOPPING', 'NOT', 'DRINKTYPE', 'NUMBER', 'QUANTITY'}\n"
     ]
    }
   ],
   "source": [
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "negated_entities = set([word[1:]\n",
    "               for t in negated_topics for word in t.split() if word.isupper()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negated_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COMPLEX_NOT_TOPPING',\n",
       " 'COMPLEX_TOPPING',\n",
       " 'CONTAINERTYPE',\n",
       " 'DRINKORDER',\n",
       " 'DRINKTYPE',\n",
       " 'NOT',\n",
       " 'NOT_QUANTITY',\n",
       " 'NOT_STYLE',\n",
       " 'NOT_TOPPING',\n",
       " 'NUMBER',\n",
       " 'ORDER',\n",
       " 'PIZZAORDER',\n",
       " 'QUANTITY',\n",
       " 'SIZE',\n",
       " 'STYLE',\n",
       " 'TOPPING'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negated_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TOPPING', 'NUMBER', 'SIZE', 'NOT_TOPPING', 'CONTAINERTYPE', 'NOT_QUANTITY', 'QUANTITY', 'STYLE', 'NOT_STYLE', 'DRINKTYPE'}\n"
     ]
    }
   ],
   "source": [
    "entities_copy = negated_entities.copy()\n",
    "entities_copy.remove('COMPLEX_NOT_TOPPING')\n",
    "entities_copy.remove('COMPLEX_TOPPING')\n",
    "entities_copy.remove('PIZZAORDER')\n",
    "entities_copy.remove('NOT')\n",
    "entities_copy.remove('DRINKORDER')\n",
    "entities_copy.remove('ORDER')\n",
    "print(entities_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('negated_entities.npy', list(entities_copy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens=np.load('tokenized_sentences_no_conct.npy', allow_pickle=True)\n",
    "tokens=np.load('dev_tokenized_sentences_no_conct.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2456446"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'want',\n",
       " 'to',\n",
       " 'order',\n",
       " 'two',\n",
       " 'medium',\n",
       " 'pizza',\n",
       " '##s',\n",
       " 'with',\n",
       " 'sausage',\n",
       " 'and',\n",
       " 'black',\n",
       " 'olive',\n",
       " '##s',\n",
       " 'and',\n",
       " 'two',\n",
       " 'medium',\n",
       " 'pizza',\n",
       " '##s',\n",
       " 'with',\n",
       " 'pepper',\n",
       " '##oni',\n",
       " 'and',\n",
       " 'extra',\n",
       " 'cheese',\n",
       " 'and',\n",
       " 'three',\n",
       " 'large',\n",
       " 'pizza',\n",
       " '##s',\n",
       " 'with',\n",
       " 'pepper',\n",
       " '##oni',\n",
       " 'and',\n",
       " 'sausage']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.str_('(ORDER (PIZZAORDER pie with (TOPPING banana pepper ) and (TOPPING peppperonis ) and (COMPLEX_TOPPING (QUANTITY extra ) (TOPPING low fat cheese ) ) ) )')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negated_topics[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'extra': 'QUANTITY',\n",
       " 'large': 'SIZE',\n",
       " 'a': 'NUMBER',\n",
       " 'olives': 'TOPPING',\n",
       " 'cheese': 'TOPPING',\n",
       " 'chicken': 'TOPPING'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regex = re.compile(r'^[^\\)]*\\)')\n",
    "\n",
    "# def extract_mapping(topic):\n",
    "#     topic=topic.replace(\n",
    "#     'COMPLEX_TOPPING', 'COMPLEX_T')\n",
    "#     mapping={}\n",
    "#     for entity in entities_copy:\n",
    "#         splitted=topic.split(entity)\n",
    "#         splitted=splitted[1:]\n",
    "#         for el in splitted:\n",
    "#             word=el.strip()\n",
    "#             word=regex.search(word).group()[:-2]\n",
    "#             for w in word.split():\n",
    "#                 mapping[w]=entity\n",
    "#     return mapping\n",
    "            \n",
    "\n",
    "# # extract_mapping(negated_topics[15])\n",
    "# extract_mapping(\n",
    "#     \"(ORDER i need (PIZZAORDER (NUMBER a ) (SIZE large ) pizza and i want (TOPPING olives ) and (COMPLEX_TOPPING (QUANTITY extra ) (TOPPING cheese ) ) as well as (TOPPING chicken ) ) on it thanks a lot )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'QUANTITY', 'SIZE', 'DRINKORDER', 'PIZZAORDER', 'STYLE', 'ORDER', 'VOLUME', 'NOT_STYLE', 'CONTAINERTYPE', 'DRINKTYPE', 'NOT_TOPPING', 'NUMBER', 'TOPPING', 'COMPLEX_TOPPING', 'NOT'}\n"
     ]
    }
   ],
   "source": [
    "print(negated_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'would', 'like', 'a', 'party', 'sized', 'high', 'rise', 'dough', 'pie', 'with', 'a', 'lot', 'of', 'banana', 'pepper', 'and', 'pe', '##cor', '##ino', 'cheese']\n",
      "i -> O\n",
      "would -> O\n",
      "like -> O\n",
      "a -> NUMBER\n",
      "party -> SIZE\n",
      "sized -> SIZE\n",
      "high -> STYLE\n",
      "rise -> STYLE\n",
      "dough -> STYLE\n",
      "pie -> O\n",
      "with -> O\n",
      "lot -> QUANTITY\n",
      "of -> QUANTITY\n",
      "banana -> TOPPING\n",
      "pepper -> TOPPING\n",
      "and -> O\n",
      "pe -> O\n",
      "pecorino -> TOPPING\n",
      "cheese -> TOPPING\n"
     ]
    }
   ],
   "source": [
    "# def map_tokens_to_entities(tokens, mapping):\n",
    "#     result = []  # List to store the token-entity pairs\n",
    "#     buffer = \"\"  # Buffer to combine subwords\n",
    "\n",
    "#     for token in tokens:\n",
    "#         if token.startswith(\"##\"):  # Handle subwords\n",
    "#             buffer += token[2:]  # Append subword to the buffer\n",
    "#         else:\n",
    "#             if buffer:  # If there's a buffer, process it\n",
    "#                 combined_word = buffer  # The completed word\n",
    "#                 entity = mapping.get(combined_word, \"O\")  # Get entity or \"O\"\n",
    "#                 result.append((combined_word, entity))\n",
    "#                 buffer = \"\"  # Clear the buffer\n",
    "\n",
    "#             buffer = token  # Start a new word with the current token if not a subword\n",
    "\n",
    "#             # Directly map the token if it's not part of a subword\n",
    "#             if not token.startswith(\"##\"):\n",
    "#                 entity = mapping.get(token, \"O\")\n",
    "#                 result.append((token, entity))\n",
    "\n",
    "#     # Process the last buffer if any\n",
    "#     if buffer:\n",
    "#         entity = mapping.get(buffer, \"O\")\n",
    "#         result.append((buffer, entity))\n",
    "\n",
    "#     # Ensure no duplicates\n",
    "#     seen = set()\n",
    "#     deduplicated_result = []\n",
    "#     for word, entity in result:\n",
    "#         if word not in seen:\n",
    "#             seen.add(word)\n",
    "#             deduplicated_result.append((word, entity))\n",
    "\n",
    "#     return deduplicated_result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(tokens[15])\n",
    "# pre_mapping=map_tokens_to_entities(tokens[15], extract_mapping(negated_topics[15]))\n",
    "# for word, entity in pre_mapping:\n",
    "#     print(f\"{word} -> {entity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('banana', 'O'),\n",
       " ('pepper', 'O'),\n",
       " ('and', 'TOPPING'),\n",
       " ('pep', 'O'),\n",
       " ('##pper', 'O'),\n",
       " ('##onis', 'O'),\n",
       " ('and', 'TOPPING'),\n",
       " ('extra', 'TOPPING'),\n",
       " ('low', 'TOPPING'),\n",
       " ('fat', 'TOPPING'),\n",
       " ('cheese', 'TOPPING')]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def get_entity_mapping(tokens,pre_mapping):\n",
    "#     result=[]\n",
    "#     continoued_mapping=None\n",
    "#     for token, mapping in zip(reversed(tokens), reversed(pre_mapping)):\n",
    "#         if continoued_mapping:\n",
    "#             result.append((token,continoued_mapping))\n",
    "#             if not token.startswith(\"##\"):\n",
    "#                 continoued_mapping=None\n",
    "#         elif token.startswith(\"##\"):\n",
    "#             result.append((token,mapping[1]))\n",
    "#             continoued_mapping=mapping[1]\n",
    "#         else:\n",
    "#             result.append((token, mapping[1]))\n",
    "            \n",
    "#     return list(reversed(result))\n",
    "\n",
    "# get_entity_mapping(tokens[5],pre_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,topic in enumerate(topics):\n",
    "#     if(topic.count(\"NOT\")>0):\n",
    "#         print(i,topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('let', 'O'), ('me', 'O'), ('have', 'O'), ('an', 'O'), ('order', 'O'), ('of', 'O'), ('two', 'NUMBER'), ('small', 'SIZE'), ('pizzas', 'O'), ('with', 'O'), ('black', 'TOPPING'), ('olives', 'TOPPING'), ('and', 'O'), ('jalapeno', 'TOPPING'), ('peppers', 'TOPPING')]\n"
     ]
    }
   ],
   "source": [
    "def get_words_to_entity(topic,mapping):\n",
    "    # print(mapping)\n",
    "    words_array=topic.split()\n",
    "    result=[]\n",
    "    current_entity='O'\n",
    "    for word in words_array:\n",
    "        if word.startswith(\"(\"):\n",
    "            if word[1:] in mapping:\n",
    "                current_entity=word[1:]\n",
    "            else:\n",
    "                current_entity='O'\n",
    "        elif word.startswith(\")\"):\n",
    "            current_entity='O'\n",
    "        else:\n",
    "            result.append((word,current_entity))\n",
    "    \n",
    "    return result\n",
    "        \n",
    "\n",
    "\n",
    "mp=get_words_to_entity(negated_topics[200],entities_copy)\n",
    "print(mp)\n",
    "# print(tokens[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CONTAINERTYPE',\n",
       " 'DRINKTYPE',\n",
       " 'NOT_QUANTITY',\n",
       " 'NOT_STYLE',\n",
       " 'NOT_TOPPING',\n",
       " 'NUMBER',\n",
       " 'QUANTITY',\n",
       " 'SIZE',\n",
       " 'STYLE',\n",
       " 'TOPPING'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ORDER can i have (PIZZAORDER (NUMBER three ) pizzas with (TOPPING peppers ) and (TOPPING chicken ) but without adding (NOT (NOT_TOPPING ham ) ) ) )\n",
      "['O', 'O', 'O', 'B-NUMBER', 'O', 'O', 'B-TOPPING', 'O', 'B-TOPPING', 'O', 'O', 'O', 'B-NOT_TOPPING']\n"
     ]
    }
   ],
   "source": [
    "def get_entity(topic,mapping):\n",
    "    # print(mapping)\n",
    "    words_array=topic.split()\n",
    "    result=[]\n",
    "    current_entity='O'\n",
    "    first=False\n",
    "    for word in words_array:\n",
    "        if word.startswith(\"(\"):\n",
    "            if word[1:] in mapping:\n",
    "                current_entity=word[1:]\n",
    "                first=True\n",
    "            else:\n",
    "                current_entity='O'\n",
    "        elif word.startswith(\")\"):\n",
    "            current_entity='O'\n",
    "        else:\n",
    "            if current_entity=='O':\n",
    "                result.append(current_entity)\n",
    "            else:\n",
    "                if first:\n",
    "                    result.append('B-'+current_entity)\n",
    "                    first=False\n",
    "                else:\n",
    "                    result.append('I-'+current_entity)    \n",
    "    return result\n",
    "        \n",
    "\n",
    "number=300\n",
    "print(negated_topics[number])\n",
    "mp=get_entity(negated_topics[number],entities_copy)\n",
    "print(mp)\n",
    "# print(tokens[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ORDER let me have an order of (PIZZAORDER (NUMBER two ) (SIZE small ) pizzas with (TOPPING black olives ) and (TOPPING jalapeno peppers ) ) )\n"
     ]
    }
   ],
   "source": [
    "print(negated_topics[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m entities_to_words_not_processed\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_entity\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnegated_topics\u001b[49m\u001b[43m,\u001b[49m\u001b[43mentities_copy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/mohamed/AC6030326030059C/CMP1Materials/Forth/First/NLP/Project/dataset/venv/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2397\u001b[0m, in \u001b[0;36mvectorize.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_stage_2(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m-> 2397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_as_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/mohamed/AC6030326030059C/CMP1Materials/Forth/First/NLP/Project/dataset/venv/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2390\u001b[0m, in \u001b[0;36mvectorize._call_as_normal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2387\u001b[0m     vargs \u001b[38;5;241m=\u001b[39m [args[_i] \u001b[38;5;28;01mfor\u001b[39;00m _i \u001b[38;5;129;01min\u001b[39;00m inds]\n\u001b[1;32m   2388\u001b[0m     vargs\u001b[38;5;241m.\u001b[39mextend([kwargs[_n] \u001b[38;5;28;01mfor\u001b[39;00m _n \u001b[38;5;129;01min\u001b[39;00m names])\n\u001b[0;32m-> 2390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vectorize_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/mohamed/AC6030326030059C/CMP1Materials/Forth/First/NLP/Project/dataset/venv/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2483\u001b[0m, in \u001b[0;36mvectorize._vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2480\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ufunc(\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mnout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 2483\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43motypes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2485\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([asanyarray(x, dtype\u001b[38;5;241m=\u001b[39mt)\n\u001b[1;32m   2486\u001b[0m                  \u001b[38;5;28;01mfor\u001b[39;00m x, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(outputs, otypes)])\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence"
     ]
    }
   ],
   "source": [
    "entities_to_words_not_processed=np.vectorize(get_entity)(negated_topics,entities_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('let', 'O'),\n",
       " ('me', 'O'),\n",
       " ('have', 'O'),\n",
       " ('an', 'O'),\n",
       " ('order', 'O'),\n",
       " ('of', 'O'),\n",
       " ('two', 'NUMBER'),\n",
       " ('small', 'SIZE'),\n",
       " ('pizza', 'O'),\n",
       " ('##s', 'O'),\n",
       " ('with', 'O'),\n",
       " ('black', 'TOPPING'),\n",
       " ('olive', 'TOPPING'),\n",
       " ('##s', 'TOPPING'),\n",
       " ('and', 'O'),\n",
       " ('ja', 'TOPPING'),\n",
       " ('##la', 'TOPPING'),\n",
       " ('##pen', 'TOPPING'),\n",
       " ('##o', 'TOPPING'),\n",
       " ('peppers', 'TOPPING')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tokens_to_entity(tokens,topic,entities):\n",
    "    mp = get_words_to_entity(topic,entities)\n",
    "    token_idx=len(tokens)-1\n",
    "    mp_idx=len(mp)-1\n",
    "    mp_entities=[entity for word,entity in mp]\n",
    "    result=[]\n",
    "    current_entity=None\n",
    "    while token_idx>=0:\n",
    "        t=tokens[token_idx]\n",
    "        if current_entity != None:\n",
    "            result.append((t,current_entity))\n",
    "            if not t.startswith(\"##\"):\n",
    "                current_entity=None\n",
    "                mp_idx-=1\n",
    "        elif t.startswith(\"##\"):\n",
    "            result.append((t,mp_entities[mp_idx]))\n",
    "            current_entity=mp_entities[mp_idx]\n",
    "        else:\n",
    "            result.append((t,mp_entities[mp_idx]))\n",
    "            mp_idx-=1\n",
    "        token_idx-=1\n",
    "    return list(reversed(result))\n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "get_tokens_to_entity(tokens[200],negated_topics[200],entities_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping tokens to entities: 348it [00:00, 33399.34it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens_to_entities = [\n",
    "    get_tokens_to_entity(tokens,topic,entities_copy) for tokens,topic in tqdm(zip(tokens,negated_topics), desc=\"Mapping tokens to entities\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_to_entities=np.array(tokens_to_entities, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('tokens_to_entities.npy', tokens_to_entities)\n",
    "np.save('dev_tokens_to_entities.npy', tokens_to_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2456446"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_to_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('can', 'O'),\n",
       " ('i', 'O'),\n",
       " ('have', 'O'),\n",
       " ('a', 'NUMBER'),\n",
       " ('large', 'SIZE'),\n",
       " ('bb', 'TOPPING'),\n",
       " ('##q', 'TOPPING'),\n",
       " ('pulled', 'TOPPING'),\n",
       " ('pork', 'TOPPING')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_entities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
