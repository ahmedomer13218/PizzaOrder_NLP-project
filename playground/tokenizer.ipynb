{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_sentences = np.load('processed_sentences.npy', allow_pickle=True)\n",
    "# processed_sentences = np.load(\n",
    "#     'processed_sentences_no_conct.npy', allow_pickle=True)\n",
    "# processed_sentences = np.load('processed_sentences_no_conct.npy', allow_pickle=True)\n",
    "# t=np.load('entities_to_words_not_processed.npy', allow_pickle=True)\n",
    "dev_processed_sentences = np.load('dev_processed_sentences_no_conct.npy', allow_pickle=True)\n",
    "dev_t=np.load('dev_entities_to_words_not_processed.npy', allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'NUMBER', 'SIZE', 'TOPPING', 'TOPPING', 'TOPPING']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.str_('can i have a large bbq pulled pork')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'need',\n",
       " 'to',\n",
       " 'order',\n",
       " 'one',\n",
       " 'large',\n",
       " 'pizza',\n",
       " 'with',\n",
       " 'ham',\n",
       " 'bacon',\n",
       " 'onions',\n",
       " 'and',\n",
       " 'black',\n",
       " 'olives',\n",
       " 'one',\n",
       " 'medium',\n",
       " 'pizza',\n",
       " 'with',\n",
       " 'sausage',\n",
       " 'and',\n",
       " 'onions',\n",
       " 'and',\n",
       " 'six',\n",
       " 'large',\n",
       " 'cokes']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_sentences[5].tolist().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sentences, labels, tokenizer, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in tqdm(sentences, desc=\"Tokenizing sentences\"):\n",
    "        encoding = tokenizer(sentence,\n",
    "                             truncation=True,\n",
    "                             padding='max_length',\n",
    "                             max_length=max_length,\n",
    "                             return_tensors=\"tf\")  # Use \"tf\" to return TensorFlow tensors\n",
    "\n",
    "        input_ids.append(encoding[\"input_ids\"])  # TensorFlow tensor\n",
    "        attention_masks.append(encoding[\"attention_mask\"])  # TensorFlow tensor\n",
    "    \n",
    "    # Convert the list of tensors into a single tensor (batch_size, sequence_length)\n",
    "    input_ids = tf.concat(input_ids, axis=0)\n",
    "    attention_masks = tf.concat(attention_masks, axis=0)\n",
    "\n",
    "    # Convert labels to a TensorFlow tensor as well\n",
    "    labels = tf.convert_to_tensor(labels)\n",
    "\n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities=np.load('full_negate_entities.npy', allow_pickle=True)\n",
    "entities_id = {e.item(): i+1 for i, e in enumerate(entities)}\n",
    "entities_id['0']=0\n",
    "entities_id['O']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_labels(labels, max_len):\n",
    "    for i in range(len(labels)):\n",
    "        if len(labels[i]) < max_len:\n",
    "            labels[i] = labels[i] + ['0'] * (max_len - len(labels[i]))\n",
    "        else:\n",
    "            labels[i] = labels[i][:max_len]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[['0']+[entities_id[i] for i in tq]+['0'] for tq in t]\n",
    "padded_labels = padding_labels(labels, 30)\n",
    "padded_labels=np.array(padded_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dev=[['0']+[entities_id[i] for i in tq]+['0'] for tq in dev_t]\n",
    "padded_labels_dev = padding_labels(labels_dev, 30)\n",
    "padded_labels_dev=np.array(padded_labels_dev, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0, 15,  2,  7,  7,  7,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing sentences: 100%|██████████| 2456446/2456446 [09:59<00:00, 4099.97it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids_train, attention_masks_train, padded_labels_train = prepare_data(processed_sentences, padded_labels, tokenizer, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2456446, 30)\n",
      "(2456446, 30)\n",
      "(2456446, 30)\n"
     ]
    }
   ],
   "source": [
    "print(input_ids_train.shape)\n",
    "print(attention_masks_train.shape)\n",
    "print(padded_labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('input_ids_train.npy', input_ids_train)\n",
    "np.save('attention_masks_train.npy', attention_masks_train)\n",
    "np.save('padded_labels_train.npy', padded_labels_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing sentences: 100%|██████████| 348/348 [00:00<00:00, 2615.07it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids_dev, attention_masks_dev, padded_labels_dev = prepare_data(dev_processed_sentences, padded_labels_dev, tokenizer, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(348, 30)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(348, 30)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(348, 30)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "print(input_ids_dev.shape)\n",
    "print(type(input_ids_dev[0]))\n",
    "print(attention_masks_dev.shape)\n",
    "print(type(attention_masks_dev[0]))\n",
    "print(padded_labels_dev.shape)\n",
    "print(type(padded_labels_dev[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('input_ids_dev.npy', input_ids_dev)\n",
    "np.save('attention_masks_dev.npy', attention_masks_dev)\n",
    "np.save('padded_labels_dev.npy', padded_labels_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([101, 2064, 1045, 2031, 1037, 2312, 22861, 4160, 2766, 15960, 102,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_ids_train[0]\n",
    "# attention_masks_train[0]\n",
    "# padded_labels_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokeninzing Sentences:   0%|          | 0/348 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokeninzing Sentences: 100%|██████████| 348/348 [00:00<00:00, 6005.74it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = [\n",
    "    tokenizer.tokenize(sentence) for sentence in tqdm(processed_sentences, desc=\"Tokeninzing Sentences\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences_numpy= np.array(tokenized_sentences, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('tokenized_sentences.npy', tokenized_sentences_numpy)\n",
    "# np.save('tokenized_sentences_no_conct.npy', tokenized_sentences_numpy)\n",
    "np.save('dev_tokenized_sentences_no_conct.npy', tokenized_sentences_numpy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
