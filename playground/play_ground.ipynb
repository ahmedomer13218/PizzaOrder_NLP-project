{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 22:23:52.834811: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734207832.886588   11481 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734207832.902587   11481 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-14 22:23:53.026278: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# import contractions\n",
    "# from spellchecker import SpellChecker\n",
    "# from transformers import BertTokenizer\n",
    "# import spacy\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "# import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"I'm learning Python and I can't wait to code!\"\n",
    "\n",
    "# expanded_text = contractions.fix(text)\n",
    "# print(expanded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spell = SpellChecker()\n",
    "\n",
    "# def spell_check(sentence):\n",
    "#     words = sentence.split()\n",
    "#     corrected_words = [spell.correction(word) or word for word in words]\n",
    "#     return ' '.join(corrected_words)\n",
    "\n",
    "\n",
    "# print(spell_check('hellop piza pepronie cheece'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokens = tokenizer.tokenize(\"Barack Obama was born in Hawaii.\")\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# doc = nlp(\"The cats are running quickly and playing with the ball.\")\n",
    "\n",
    "# for token in doc:\n",
    "#     print(f\"Original: {token.text}, Lemmatized: {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from word2number import w2n\n",
    "\n",
    "# print(w2n.word_to_num(\"two hundred seventy-five pizzas\"))\n",
    "# print(w2n.word_to_num(\"one thousand five hundred twenty-four\"))\n",
    "# print(w2n.word_to_num(\"one million five hundred twenty-four\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from num2words import num2words\n",
    "\n",
    "# # Convert number to words\n",
    "# number = 1234\n",
    "# word = num2words(number)\n",
    "# print(word)  # Output: 'one thousand, two hundred and thirty-four'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = re.compile(r\"[^\\w\\s-]\")\n",
    "def preprocess_text(text):\n",
    "\n",
    "\n",
    "    # text = contractions.fix(text) # expand contractions //stop now to save time annotating\n",
    "\n",
    "\n",
    "    # text = spell_check(text) # spell check\n",
    "    # words = text.split()\n",
    "    # corrected_words = [spell.correction(word) or word for word in words]\n",
    "    # text = ' '.join(corrected_words)\n",
    "\n",
    "\n",
    "    text = tf.strings.strip(text) # leading/trailing whitespace removal\n",
    "\n",
    "\n",
    "    text = tf.strings.lower(text)\n",
    "\n",
    "    text = punctuation.sub(\"\", text.numpy().decode('utf-8')) # punctuation removal\n",
    "\n",
    "\n",
    "    # text = nlp(text) # lemmatization\n",
    "\n",
    "\n",
    "    # text = tokenizer.tokenize(text.numpy().decode('utf-8'))  # tokenization\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2456446\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "with open('PIZZA_train.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev.SRC': 'i want to order two medium pizzas with sausage and black olives and two medium pizzas with pepperoni and extra cheese and three large pizzas with pepperoni and sausage', 'dev.EXR': '(ORDER (PIZZAORDER (NUMBER 2 ) (SIZE MEDIUM ) (COMPLEX_TOPPING (QUANTITY EXTRA ) (TOPPING CHEESE ) ) (TOPPING PEPPERONI ) ) (PIZZAORDER (NUMBER 2 ) (SIZE MEDIUM ) (TOPPING OLIVES ) (TOPPING SAUSAGE ) ) (PIZZAORDER (NUMBER 3 ) (SIZE LARGE ) (TOPPING PEPPERONI ) (TOPPING SAUSAGE ) ) )', 'dev.TOP': '(ORDER i want to order (PIZZAORDER (NUMBER two ) (SIZE medium ) pizzas with (TOPPING sausage ) and (TOPPING black olives ) ) and (PIZZAORDER (NUMBER two ) (SIZE medium ) pizzas with (TOPPING pepperoni ) and (COMPLEX_TOPPING (QUANTITY extra ) (TOPPING cheese ) ) ) and (PIZZAORDER (NUMBER three ) (SIZE large ) pizzas with (TOPPING pepperoni ) and (TOPPING sausage ) ) )', 'dev.PCFG_ERR': 'False'}\n"
     ]
    }
   ],
   "source": [
    "# with open('PIZZA_dev.json', 'r') as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# # Print the loaded data\n",
    "# print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(data):\n",
    "    return data['train.SRC'], data['train.EXR'], data['train.TOP'], data['train.TOP-DECOUPLED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_text(data):\n",
    "#     return data['dev.SRC'], data['dev.EXR'], data['dev.TOP'], data['dev.PCFG_ERR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, explanations, topics, decoupled_topics = map(np.array, zip(*map(extract_text, data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd_sentences = pd.Series(sentences)\n",
    "\n",
    "# # tqdm.pandas(desc=\"Processing Data\")\n",
    "\n",
    "\n",
    "# # processed_pd_sentences = pd_sentences.progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.DataFrame(\n",
    "#     {'sentence': pd_sentences,\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['contractions']=df['sentence'].apply(lambda x: contractions.fix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['strip']=df['contractions'].apply(lambda x: tf.strings.strip(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['spell_check']=df['contractions'].apply(lambda x: spell_check(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['lower'] = df['sentence'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['no_punctuation'] = df['lower'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences[2], explanations[2], topics[2], decoupled_topics[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.convert_to_tensor(tokenizer.tokenize(p.numpy().decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(preprocess_text(sentences[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np_sentences, np_explanations, np_topics, np_decoupled_topics = map(np.array, [sentences, explanations, topics, decoupled_topics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:   0%|          | 0/348 [00:00<?, ?it/s]I0000 00:00:1733687564.477463  151078 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1176 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "Processing Sentences: 100%|██████████| 348/348 [00:00<00:00, 1513.22it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "processed_sentences = [\n",
    "    preprocess_text(sentence) for sentence in tqdm(sentences, desc=\"Processing Sentences\")\n",
    "]\n",
    "processed_sentences = np.array(processed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('processed_sentences.npy', processed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# np.save('processed_sentences_no_conct.npy', processed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('dev_processed_sentences_no_conct.npy', processed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_sentences = np.vectorize(preprocess_text)(np_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'would', 'like', 'a', 'large', 'vegetarian', 'pizza']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(processed_sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.str_(\"(ORDER i'd like (PIZZAORDER (NUMBER a ) (SIZE large ) (STYLE vegetarian ) pizza ) )\")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.str_('(ORDER (PIZZAORDER (NUMBER 1 ) (SIZE LARGE ) (STYLE VEGETARIAN ) ) )')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ORDER (PIZZAORDER pie with (TOPPING banana pepper ) and (TOPPING peppperonis ) and (COMPLEX_TOPPING (QUANTITY extra ) (TOPPING low fat cheese ) ) ) )\n",
      "{'TOPPING': ['banana pepper ', 'peppperonis ', 'low fat cheese '], 'COMPLEX_TOPPING': ['(QUANTITY extra ']}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "target = topics[5][1:-1].split('PIZZAORDER')[1]\n",
    "\n",
    "\"(NUMBER 5 ) (SIZE MEDIUM ) (TOPPING HAM ) (TOPPING TOMATOES )  \"\n",
    "\n",
    "entities = defaultdict(list)\n",
    "\n",
    "pattern = r'\\((\\w+)\\s+([^\\)]+)\\)'\n",
    "\n",
    "matches = re.findall(pattern, target)\n",
    "\n",
    "for match in matches:\n",
    "    entity_type, value = match\n",
    "    entities[entity_type].append(value)\n",
    "\n",
    "print(topics[5])\n",
    "print(dict(entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import regex as re\n",
    "\n",
    "# def extract_parentheses(text):\n",
    "#     # Regex to match content inside parentheses with recursion\n",
    "#     pattern = r'\\((?:[^()]+|(?R))*\\)'  # Match outer and inner parentheses, including recursion\n",
    "#     matches = re.findall(pattern, text)\n",
    "    \n",
    "#     # Remove outer parentheses from each match\n",
    "#     matches = [match[1:-1] for match in matches]\n",
    "    \n",
    "#     return matches\n",
    "\n",
    "# # Example usage\n",
    "# target = \"(ORDER i need to order (PIZZAORDER (NUMBER one ) (SIZE large ) (STYLE vegetarian ) pizza with (COMPLEX_TOPPING (QUANTITY extra ) (TOPPING banana peppers ) ) ) )\"\n",
    "\n",
    "# matches = extract_parentheses(target)\n",
    "# for match in matches:\n",
    "#     print(match)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NUMBER one )\n",
      "(SIZE large )\n",
      "(STYLE vegetarian )\n",
      "(COMPLEX_TOPPING (QUANTITY extra ) (TOPPING banana peppers ) )\n"
     ]
    }
   ],
   "source": [
    "# import regex as re\n",
    "\n",
    "# # Sample input string\n",
    "# target = \"(NUMBER one ) (SIZE large ) (STYLE vegetarian ) pizza with (COMPLEX_TOPPING (QUANTITY extra ) (TOPPING banana peppers ) )\"\n",
    "\n",
    "# # Initialize a dictionary to store the word-to-label mapping\n",
    "# word_to_label = {}\n",
    "\n",
    "# # Regular expression to capture entities and their corresponding values\n",
    "# pattern = r'\\((?:[^()]*|(?R))+\\)'\n",
    "\n",
    "# # Find all matches of the pattern\n",
    "# matches = re.findall(pattern, target)\n",
    "# for match in matches:\n",
    "#     print(match)\n",
    "\n",
    "# # # Loop through matches and populate the word_to_label dictionary\n",
    "# # for match in matches:\n",
    "# #     entity_type, value = match\n",
    "# #     # Split the value into individual words\n",
    "# #     words = value.split()\n",
    "# #     for word in words:\n",
    "# #         # Map each word to the corresponding entity type (label)\n",
    "# #         word_to_label[word] = entity_type\n",
    "\n",
    "# # Output the word-to-label dictionary\n",
    "# # print(word_to_label.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_nested_parentheses(s):\n",
    "#     stack = []\n",
    "#     result = []\n",
    "#     start = 0  # To keep track of the starting index of each matched parentheses\n",
    "\n",
    "#     for i, char in enumerate(s):\n",
    "#         if char == '(':\n",
    "#             stack.append(i)  # Push the index of '(' onto the stack\n",
    "#         elif char == ')':\n",
    "#             stack.pop()  # Pop the last '(' from the stack\n",
    "#             if not stack:\n",
    "#                 # We have a complete match, extract it\n",
    "#                 result.append(s[start:i+1])\n",
    "#                 start = i + 1  # Update the start position for the next potential match\n",
    "\n",
    "#     return result\n",
    "\n",
    "\n",
    "# # Sample input string\n",
    "# target = \"(ORDER i need to order (PIZZAORDER (NUMBER one ) (SIZE large ) (STYLE vegetarian ) pizza with (COMPLEX_TOPPING (QUANTITY extra ) (TOPPING banana peppers ) ) ) )\"\n",
    "\n",
    "# # Extract nested parentheses\n",
    "# matches = extract_nested_parentheses(target)\n",
    "\n",
    "# # Output the matched strings\n",
    "# print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ORDER can i have (PIZZAORDER (NUMBER one ) (STYLE high rise dough ) pie with (TOPPING american cheese ) and (COMPLEX_TOPPING (QUANTITY a lot of ) (TOPPING meatball ) ) ) )\n",
      "['NUMBER one ', 'STYLE high rise dough ', 'TOPPING american cheese ', 'QUANTITY a lot of ', 'TOPPING meatball ']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_outer_parentheses(text):\n",
    "    # This regex captures the outermost parentheses pair\n",
    "    pattern = r'\\(([^()]*(?:\\((?:[^()]*)\\)[^()]*)*)\\)'\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "# def recursive_extract_outer_parentheses(text):\n",
    "    \n",
    "def get_result(text):\n",
    "    queue=[text]\n",
    "    result=[]\n",
    "    while queue:\n",
    "        current = queue.pop(0)\n",
    "        matches = extract_outer_parentheses(current)\n",
    "        for match in matches:\n",
    "            if '(' in match:\n",
    "                queue.append(match)\n",
    "            else:\n",
    "                result.append(match)\n",
    "    return result\n",
    "    \n",
    "    \n",
    "\n",
    "target = topics[12]\n",
    "\n",
    "# matches = extract_outer_parentheses(target)\n",
    "matches = get_result(target)\n",
    "print(target)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER a \n",
      "SIZE large \n",
      "TOPPING olives \n",
      "TOPPING chicken \n",
      "QUANTITY extra \n",
      "TOPPING cheese \n"
     ]
    }
   ],
   "source": [
    "matches = get_result(\"(ORDER i need (PIZZAORDER (NUMBER a ) (SIZE large ) pizza and i want (TOPPING olives ) and (COMPLEX_TOPPING (QUANTITY extra ) (TOPPING cheese ) ) as well as (TOPPING chicken ) ) on it thanks a lot )\")\n",
    "for match in matches:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=set([word[1:] for t in topics[12:15] for word in t.split() if word.isupper()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COMPLEX_TOPPING',\n",
       " 'NOT',\n",
       " 'NUMBER',\n",
       " 'ORDER',\n",
       " 'PIZZAORDER',\n",
       " 'QUANTITY',\n",
       " 'SIZE',\n",
       " 'STYLE',\n",
       " 'TOPPING'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
