{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mohamed/AC6030326030059C/CMP1Materials/Forth/First/NLP/Project/dataset/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-15 14:22:20.410925: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734265340.468493   74172 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734265340.485880   74172 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-15 14:22:20.615300: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_sentences_dev = np.load('processed_sentences_no_conct_dev.npy', allow_pickle=True)\n",
    "# words_to_entitied_dev=np.load('words_to_entities_dev.npy', allow_pickle=True)\n",
    "processed_sentences_train = np.load('processed_sentences_no_conct_train.npy', allow_pickle=True)\n",
    "words_to_entitied_train=np.load('words_to_entities_train.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sentences, labels, tokenizer, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    with tf.device('/CPU:0'):\n",
    "        for sentence in tqdm(sentences, desc=\"Tokenizing sentences\"):\n",
    "            encoding = tokenizer(sentence,\n",
    "                                 truncation=True,\n",
    "                                 padding='max_length',\n",
    "                                 max_length=max_length,\n",
    "                                 return_tensors=\"tf\")  # Use \"tf\" to return TensorFlow tensors\n",
    "\n",
    "            input_ids.append(encoding[\"input_ids\"])  # TensorFlow tensor\n",
    "            attention_masks.append(encoding[\"attention_mask\"])  # TensorFlow tensor\n",
    "    \n",
    "    # Convert the list of tensors into a single tensor (batch_size, sequence_length)\n",
    "    input_ids = tf.concat(input_ids, axis=0)\n",
    "    attention_masks = tf.concat(attention_masks, axis=0)\n",
    "\n",
    "    # Convert labels to a TensorFlow tensor as well\n",
    "    labels = tf.convert_to_tensor(labels)\n",
    "\n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['QUANTITY', 'STYLE', 'SIZE', 'TOPPING', 'NOT_VOLUME',\n",
       "       'NOT_TOPPING', 'NOT_SIZE', 'NUMBER', 'NOT_STYLE', 'VOLUME',\n",
       "       'CONTAINERTYPE', 'NOT_CONTAINERTYPE', 'NOT_NUMBER',\n",
       "       'NOT_DRINKTYPE', 'DRINKTYPE', 'NOT_QUANTITY'], dtype='<U17')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities=np.load('final_negated_entites.npy', allow_pickle=True)\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_labels = [['B-'+entity, 'I-'+entity] for entity in entities]\n",
    "entity_labels = [label for sublist in entity_labels for label in sublist]\n",
    "np.save('entity_labels.npy', entity_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-QUANTITY': 1,\n",
       " 'I-QUANTITY': 2,\n",
       " 'B-STYLE': 3,\n",
       " 'I-STYLE': 4,\n",
       " 'B-SIZE': 5,\n",
       " 'I-SIZE': 6,\n",
       " 'B-TOPPING': 7,\n",
       " 'I-TOPPING': 8,\n",
       " 'B-NOT_VOLUME': 9,\n",
       " 'I-NOT_VOLUME': 10,\n",
       " 'B-NOT_TOPPING': 11,\n",
       " 'I-NOT_TOPPING': 12,\n",
       " 'B-NOT_SIZE': 13,\n",
       " 'I-NOT_SIZE': 14,\n",
       " 'B-NUMBER': 15,\n",
       " 'I-NUMBER': 16,\n",
       " 'B-NOT_STYLE': 17,\n",
       " 'I-NOT_STYLE': 18,\n",
       " 'B-VOLUME': 19,\n",
       " 'I-VOLUME': 20,\n",
       " 'B-CONTAINERTYPE': 21,\n",
       " 'I-CONTAINERTYPE': 22,\n",
       " 'B-NOT_CONTAINERTYPE': 23,\n",
       " 'I-NOT_CONTAINERTYPE': 24,\n",
       " 'B-NOT_NUMBER': 25,\n",
       " 'I-NOT_NUMBER': 26,\n",
       " 'B-NOT_DRINKTYPE': 27,\n",
       " 'I-NOT_DRINKTYPE': 28,\n",
       " 'B-DRINKTYPE': 29,\n",
       " 'I-DRINKTYPE': 30,\n",
       " 'B-NOT_QUANTITY': 31,\n",
       " 'I-NOT_QUANTITY': 32,\n",
       " '0': 0,\n",
       " 'O': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_id = {e: i+1 for i, e in enumerate(entity_labels)}\n",
    "entities_id['0']=0\n",
    "entities_id['O']=0\n",
    "entities_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_labels(labels, max_len):\n",
    "    for i in range(len(labels)):\n",
    "        if len(labels[i]) < max_len:\n",
    "            labels[i] = labels[i] + ['0'] * (max_len - len(labels[i]))\n",
    "        else:\n",
    "            labels[i] = labels[i][:max_len]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train=[['0']+[entities_id[i] for i in tq]+['0'] for tq in words_to_entitied_train]\n",
    "padded_labels_train = padding_labels(labels_train, 30)\n",
    "padded_labels_train=np.array(padded_labels_train, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dev=[['0']+[entities_id[i] for i in tq]+['0'] for tq in words_to_entitied_dev]\n",
    "padded_labels_dev = padding_labels(labels_dev, 30)\n",
    "padded_labels_dev=np.array(padded_labels_dev, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing sentences: 100%|██████████| 1000000/1000000 [04:38<00:00, 3593.54it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids_train, attention_masks_train, padded_labels_train = prepare_data(processed_sentences_train[:1000000], padded_labels_train[:1000000], tokenizer, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('input_ids_train_position1.npy', input_ids_train)\n",
    "np.save('attention_masks_train_position1.npy', attention_masks_train)\n",
    "np.save('padded_labels_train_position1.npy', padded_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734213454.995825   45598 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4273 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "Tokenizing sentences: 100%|██████████| 1000000/1000000 [05:04<00:00, 3284.11it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids_train2, attention_masks_train2, padded_labels_train2 = prepare_data(processed_sentences_train[1000000:2000000], padded_labels_train[1000000:2000000], tokenizer, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('input_ids_train_position2.npy', input_ids_train2)\n",
    "np.save('attention_masks_train_position2.npy', attention_masks_train2)\n",
    "np.save('padded_labels_train_position2.npy', padded_labels_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734214039.046564   51132 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4273 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "Tokenizing sentences: 100%|██████████| 456446/456446 [02:13<00:00, 3411.58it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids_train3, attention_masks_train3, padded_labels_train3 = prepare_data(processed_sentences_train[2000000:], padded_labels_train[2000000:], tokenizer, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('input_ids_train_position3.npy', input_ids_train3)\n",
    "np.save('attention_masks_train_position3.npy', attention_masks_train3)\n",
    "np.save('padded_labels_train_position3.npy', padded_labels_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734212110.229057   33534 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4273 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "Tokenizing sentences: 100%|██████████| 348/348 [00:00<00:00, 3049.25it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids_dev, attention_masks_dev, padded_labels_dev = prepare_data(processed_sentences_dev, padded_labels_dev, tokenizer, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('input_ids_dev_position.npy', input_ids_dev)\n",
    "np.save('attention_masks_dev_position.npy', attention_masks_dev)\n",
    "np.save('padded_labels_dev_position.npy', padded_labels_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train_position=np.concat([np.load('input_ids_train_position1.npy'), np.load('input_ids_train_position2.npy'),np.load('input_ids_train_position3.npy')], axis=0)\n",
    "attention_masks_train_position=np.concat([np.load('attention_masks_train_position1.npy'), np.load('attention_masks_train_position2.npy'),np.load('attention_masks_train_position3.npy')], axis=0)\n",
    "padded_labels_train_position=np.concat([np.load('padded_labels_train_position1.npy'), np.load('padded_labels_train_position2.npy'),np.load('padded_labels_train_position3.npy')], axis=0)\n",
    "\n",
    "np.save('input_ids_train_position.npy', input_ids_train_position)\n",
    "np.save('attention_masks_train_position.npy', attention_masks_train_position)\n",
    "np.save('padded_labels_train_position.npy', padded_labels_train_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
