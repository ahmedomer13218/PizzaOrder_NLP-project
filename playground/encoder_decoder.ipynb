{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Attention, Input, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "VOCAB_SIZE = 30522\n",
    "EMBEDDING_DIM = 128  \n",
    "MAX_LEN = 30\n",
    "NUM_CLASSES = 33\n",
    "\n",
    "def intialize_model(VOCAB_SIZE,EMBEDDING_DIM,MAX_LEN,NUM_CLASSES):\n",
    "    input_seq = Input(shape=(MAX_LEN,), dtype='int32', name=\"Input_Sequence\")\n",
    "\n",
    "    embedding = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_LEN, name=\"Embedding_Layer\")(input_seq)\n",
    "\n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = Bidirectional(\n",
    "        LSTM(128, return_sequences=True, return_state=True, name=\"Encoder_LSTM\"),\n",
    "        name=\"Bidirectional_LSTM\"\n",
    "    )(embedding)\n",
    "\n",
    "    state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\n",
    "    state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\n",
    "\n",
    "    decoder_lstm = LSTM(256, return_sequences=True, name=\"Decoder_LSTM\")\n",
    "    decoder_outputs = decoder_lstm(encoder_outputs, initial_state=[state_h, state_c])\n",
    "\n",
    "    attention = Attention(name=\"Attention_Layer\")([decoder_outputs, encoder_outputs])\n",
    "\n",
    "    combined = tf.keras.layers.Concatenate()([decoder_outputs, attention])\n",
    "\n",
    "    output = TimeDistributed(Dense(NUM_CLASSES, activation=\"softmax\"), name=\"Output_Layer\")(combined)\n",
    "\n",
    "    model = Model(inputs=input_seq, outputs=output, name=\"Encoder_Decoder_NER\")\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = intialize_model(VOCAB_SIZE,EMBEDDING_DIM,MAX_LEN,NUM_CLASSES)\n",
    "\n",
    "input_ids_train=np.load('input_ids_train_position.npy', allow_pickle=True)\n",
    "attention_masks_train=np.load('attention_masks_train_position.npy', allow_pickle=True)\n",
    "padded_labels_train=np.load('padded_labels_train_position.npy', allow_pickle=True)\n",
    "input_ids_dev=np.load('input_ids_dev_position.npy', allow_pickle=True)\n",
    "attention_masks_dev=np.load('attention_masks_dev_position.npy', allow_pickle=True)\n",
    "padded_labels_dev=np.load('padded_labels_dev_position.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "# Generate random indices for shuffling\n",
    "train_size = len(input_ids_train)\n",
    "indices = np.random.permutation(train_size)\n",
    "\n",
    "# Calculate split point (10% of train data)\n",
    "split_point = int(train_size * 0.1)\n",
    "\n",
    "# Split indices\n",
    "transfer_indices = indices[:split_point]\n",
    "\n",
    "# Transfer 10% from train to dev\n",
    "input_ids_dev = np.concatenate([input_ids_dev, input_ids_train[transfer_indices]])\n",
    "attention_masks_dev = np.concatenate([attention_masks_dev, attention_masks_train[transfer_indices]])\n",
    "padded_labels_dev = np.concatenate([padded_labels_dev, padded_labels_train[transfer_indices]])\n",
    "\n",
    "# Keep remaining 90% for train\n",
    "keep_indices = indices[split_point:]\n",
    "input_ids_train = input_ids_train[keep_indices]\n",
    "attention_masks_train = attention_masks_train[keep_indices]\n",
    "padded_labels_train = padded_labels_train[keep_indices]\n",
    "\n",
    "# Shuffle dev set\n",
    "dev_indices = np.random.permutation(len(input_ids_dev))\n",
    "input_ids_dev = input_ids_dev[dev_indices]\n",
    "attention_masks_dev = attention_masks_dev[dev_indices]\n",
    "padded_labels_dev = padded_labels_dev[dev_indices]\n",
    "\n",
    "np.save('input_ids_train_position_shuffled.npy', input_ids_train)\n",
    "np.save('attention_masks_train_position_shuffled.npy', attention_masks_train)\n",
    "np.save('padded_labels_train_position_shuffled.npy', padded_labels_train)\n",
    "np.save('input_ids_dev_position_shuffled.npy', input_ids_dev)\n",
    "np.save('attention_masks_dev_position_shuffled.npy', attention_masks_dev)\n",
    "np.save('padded_labels_dev_position_shuffled.npy', padded_labels_dev)\n",
    "\n",
    "\n",
    "# def create_tf_dataset_with_attention(input_ids, attention_mask, labels):\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": input_ids, \"attention_mask\": attention_mask}, labels))\n",
    "#     return dataset.batch(32)\n",
    "\n",
    "# def create_tf_dataset(input_ids, labels):\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices(({\"Input_Sequence\": input_ids}, labels))\n",
    "#     return dataset.batch(32)\n",
    "\n",
    "\n",
    "# train_dataset = create_tf_dataset(input_ids_train, padded_labels_train)\n",
    "# dev_dataset = create_tf_dataset(input_ids_dev, padded_labels_dev)\n",
    "\n",
    "# checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath='model_checkpoint.weights.h5',\n",
    "#     save_weights_only=True,\n",
    "#     monitor='val_loss',\n",
    "#     mode='min',\n",
    "#     save_best_only=True\n",
    "# )\n",
    "\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=1)\n",
    "\n",
    "# reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "#     monitor='val_loss',\n",
    "#     factor=0.2,\n",
    "#     patience=2,\n",
    "#     min_lr=0.00001\n",
    "# )\n",
    "\n",
    "# early_stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "#     monitor='val_loss',\n",
    "#     patience=3,\n",
    "#     restore_best_weights=True\n",
    "# )\n",
    "\n",
    "\n",
    "# history = model.fit(\n",
    "#     train_dataset,\n",
    "#     validation_data=dev_dataset,\n",
    "#     epochs=6,\n",
    "#     callbacks=[checkpoint_callback, tensorboard_callback, reduce_lr_callback, early_stop_callback]\n",
    "# )\n",
    "\n",
    "# model.save('encoder_decoder_with_positions.keras')\n",
    "\n",
    "# history_dict = history.history\n",
    "# with open('training_history_encoder_decoder.json', 'w') as f:\n",
    "#     json.dump(history_dict, f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
