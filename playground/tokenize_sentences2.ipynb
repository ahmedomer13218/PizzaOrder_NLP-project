{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mohamed/AC6030326030059C/CMP1Materials/Forth/First/NLP/Project/dataset/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-15 18:38:54.658026: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734280734.709472  125155 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734280734.725558  125155 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-15 18:38:54.849605: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_sentences_dev = np.load('processed_sentences_no_conct_dev.npy', allow_pickle=True)\n",
    "# words_to_entitied_dev=np.load('words_to_entities_dev2.npy', allow_pickle=True)\n",
    "# processed_sentences_train = np.load('processed_sentences_no_conct_train.npy', allow_pickle=True)\n",
    "# words_to_entitied_train=np.load('words_to_entities_train2.npy', allow_pickle=True)\n",
    "processed_sentences_test = np.load('processed_sentences_no_conct_test.npy', allow_pickle=True)\n",
    "words_to_entitied_test=np.load('words_to_entities_test2.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sentences, labels, tokenizer, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    with tf.device('/CPU:0'):\n",
    "        for sentence in tqdm(sentences, desc=\"Tokenizing sentences\"):\n",
    "            encoding = tokenizer(sentence,\n",
    "                                 truncation=True,\n",
    "                                 padding='max_length',\n",
    "                                 max_length=max_length,\n",
    "                                 return_tensors=\"tf\")  # Use \"tf\" to return TensorFlow tensors\n",
    "\n",
    "            input_ids.append(encoding[\"input_ids\"])  # TensorFlow tensor\n",
    "            attention_masks.append(encoding[\"attention_mask\"])  # TensorFlow tensor\n",
    "    \n",
    "    # Convert the list of tensors into a single tensor (batch_size, sequence_length)\n",
    "    input_ids = tf.concat(input_ids, axis=0)\n",
    "    attention_masks = tf.concat(attention_masks, axis=0)\n",
    "\n",
    "    # Convert labels to a TensorFlow tensor as well\n",
    "    labels = tf.convert_to_tensor(labels)\n",
    "\n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['PIZZAORDER', 'DRINKORDER'], dtype='<U10')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities=np.load('final_negated_entites2.npy', allow_pickle=True)\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_labels = [['B-'+entity, 'I-'+entity] for entity in entities]\n",
    "entity_labels = [label for sublist in entity_labels for label in sublist]\n",
    "# np.save('entity_labels2.npy', entity_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-PIZZAORDER': 1,\n",
       " 'I-PIZZAORDER': 2,\n",
       " 'B-DRINKORDER': 3,\n",
       " 'I-DRINKORDER': 4,\n",
       " '0': 0,\n",
       " 'O': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_id = {e: i+1 for i, e in enumerate(entity_labels)}\n",
    "entities_id['0']=0\n",
    "entities_id['O']=0\n",
    "entities_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_labels(labels, max_len):\n",
    "    for i in range(len(labels)):\n",
    "        if len(labels[i]) < max_len:\n",
    "            labels[i] = labels[i] + ['0'] * (max_len - len(labels[i]))\n",
    "        else:\n",
    "            labels[i] = labels[i][:max_len]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train=[['0']+[entities_id[i] for i in tq]+['0'] for tq in words_to_entitied_train]\n",
    "padded_labels_train = padding_labels(labels_train, 30)\n",
    "padded_labels_train=np.array(padded_labels_train, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734265704.372842   75172 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4273 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "Tokenizing sentences: 100%|██████████| 1000000/1000000 [04:53<00:00, 3407.05it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids_train, attention_masks_train, padded_labels_train = prepare_data(processed_sentences_train[:1000000], padded_labels_train[:1000000], tokenizer, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('input_ids_train_position21.npy', input_ids_train)\n",
    "np.save('attention_masks_train_position21.npy', attention_masks_train)\n",
    "np.save('padded_labels_train_position21.npy', padded_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734266379.689275   76478 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4273 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "Tokenizing sentences: 100%|██████████| 1000000/1000000 [05:26<00:00, 3062.23it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids_train, attention_masks_train, padded_labels_train = prepare_data(processed_sentences_train[1000000:2000000], padded_labels_train[1000000:2000000], tokenizer, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('input_ids_train_position22.npy', input_ids_train)\n",
    "np.save('attention_masks_train_position22.npy', attention_masks_train)\n",
    "np.save('padded_labels_train_position22.npy', padded_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734267016.771709   77849 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4273 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "Tokenizing sentences: 100%|██████████| 456446/456446 [02:13<00:00, 3412.17it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids_train, attention_masks_train, padded_labels_train = prepare_data(processed_sentences_train[2000000:], padded_labels_train[2000000:], tokenizer, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('input_ids_train_position23.npy', input_ids_train)\n",
    "np.save('attention_masks_train_position23.npy', attention_masks_train)\n",
    "np.save('padded_labels_train_position23.npy', padded_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dev=[['0']+[entities_id[i] for i in tq]+['0'] for tq in words_to_entitied_dev]\n",
    "padded_labels_dev = padding_labels(labels_dev, 30)\n",
    "padded_labels_dev=np.array(padded_labels_dev, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734265551.341278   74628 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4273 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "Tokenizing sentences: 100%|██████████| 348/348 [00:00<00:00, 3105.40it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids_dev, attention_masks_dev, padded_labels_dev = prepare_data(processed_sentences_dev, padded_labels_dev, tokenizer, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('input_ids_dev_position2.npy', input_ids_dev)\n",
    "np.save('attention_masks_dev_position2.npy', attention_masks_dev)\n",
    "np.save('padded_labels_dev_position2.npy', padded_labels_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train_position=np.concat([np.load('input_ids_train_position21.npy'), np.load('input_ids_train_position22.npy'),np.load('input_ids_train_position23.npy')], axis=0)\n",
    "attention_masks_train_position=np.concat([np.load('attention_masks_train_position21.npy'), np.load('attention_masks_train_position22.npy'),np.load('attention_masks_train_position23.npy')], axis=0)\n",
    "padded_labels_train_position=np.concat([np.load('padded_labels_train_position21.npy'), np.load('padded_labels_train_position22.npy'),np.load('padded_labels_train_position23.npy')], axis=0)\n",
    "\n",
    "np.save('input_ids_train_position222.npy', input_ids_train_position)\n",
    "np.save('attention_masks_train_position222.npy', attention_masks_train_position)\n",
    "np.save('padded_labels_train_position222.npy', padded_labels_train_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test=[['0']+[entities_id[i] for i in tq]+['0'] for tq in words_to_entitied_test]\n",
    "padded_labels_test = padding_labels(labels_test, 30)\n",
    "padded_labels_test=np.array(padded_labels_test, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734280822.169608  125155 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1200 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "Tokenizing sentences: 100%|██████████| 1357/1357 [00:00<00:00, 3504.87it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids_test, attention_masks_test, padded_labels_test = prepare_data(processed_sentences_test, padded_labels_test, tokenizer, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('input_ids_test_position2.npy', input_ids_test)\n",
    "np.save('attention_masks_test_position2.npy', attention_masks_test)\n",
    "np.save('padded_labels_test_position2.npy', padded_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
